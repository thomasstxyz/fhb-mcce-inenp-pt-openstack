\section{Infrastructure-as-Code Setup \& Usage}
This section explains the infrastructure code and how to use it.

\subsection{Terraform - Openstack provisioning}
To get started, the Git Repository needs to be cloned.

\begin{verbatim}
	$ git clone https://github.com/thomasstxyz/fhb-mcce-inenp-pt-openstack
\end{verbatim}

\noindent
Next, the credentials for authentication to the Openstack API,
as well as the public ssh key will be
read into environment variables.

\begin{verbatim}
	$ export OS_USERNAME=my_user
	$ export OS_PASSWORD=my_pass
	$ export OS_PROJECT_ID=1234...
	$ export TF_VAR_public_key="ssh-rsa ..."	
\end{verbatim}

\noindent
The project id can be retrieved from the Openstack Horizon Dashboard
under "Project - API Access" 
\url{http://172.20.41.1/horizon/project/api_access/},
upon pressing the button "View Credentials". \\

\noindent
Now the setup is complete and Terraform should be able to authenticate
to the Openstack API. Change into the terraform directory.

\begin{verbatim}
	$ cd terraform
\end{verbatim}

\noindent
Use Terraform plan to look at what changes would be made by executing
(this is just a dry run and does not make any changes).

\begin{verbatim}
	$ terraform plan
\end{verbatim}

\noindent
A summary will be printed in the console, stating 
how many resources to add, change and destroy. \\

\begin{verbatim}
	Plan: 21 to add, 0 to change, 0 to destroy.
\end{verbatim}

\noindent
If content with the dry run, the plan can be applied.

\begin{verbatim}
	$ terraform apply
\end{verbatim}

\noindent
Again a summary will be printed, and the user is asked to confirm with yes. \\

\noindent
After a successful Terraform apply, the IP addresses of the created compute 
instances are displayed on screen. Additionally, the addresses are written
to the Ansible inventory file \verb|../ansible/inventory|,
which is needed by Ansible for the next step. \\

\noindent
All resources can be deleted, by running Terraform destroy.

\begin{verbatim}
	$ terraform destroy
\end{verbatim}

\subsection{Ansible - Kubernetes cluster \& OS configuration}
After provisioning of the Openstack instances via Terraform,
Ansible creates a Kubernetes cluster.
For this, the Ansible Role \verb|geerlingguy.kubernetes|
(https://github.com/geerlingguy/ansible-role-kubernetes) is used. \\

\noindent
Change into the ansible directory 

\begin{verbatim}
	$ cd ansible
\end{verbatim}

\noindent
and install the project dependencies,
which is only the kubernetes role.

\begin{verbatim}
	$ ansible-galaxy role install -r roles/requirements.yml
\end{verbatim}

\noindent
Now the Ansible Playbook can be run to install dependencies,
load kernel modules, do further configuration, and finally
create a Kubernetes cluster on top of the compute instances.

\begin{verbatim}
	$ ansible-playbook -i inventory playbook.yml
\end{verbatim}

\noindent
\textbf{Note:} There are certain \verb|pre_tasks| specified in the
\verb|playbook.yml|, which are needed for this specific 
Openstack infrastructure. The hostname is mapped to \verb|127.0.1.1| 
in the \verb|/etc/hosts| file, which fixes a bug, where
every time sudo is called, the process hangs for about ten seconds,
which would significantly decrease the speed at which the 
Ansible playbook is run, because it calls sudo on every task. 
Furthermore, DNS nameservers are specified, because the Openstack DHCP server
does not seem to provide them. \\

\noindent
After successful playbook run, the Kubernetes cluster is ready for use. \\

\noindent
Get started by logging into the master node

\begin{verbatim}
	$ ssh ubuntu@<master_node_ip>
\end{verbatim}

\noindent
and switching to the root user.

\begin{verbatim}
	$ sudo su - root
\end{verbatim}

\noindent
The \verb|kubectl| command line tool can be used to interact with Kubernetes.

\begin{verbatim}
	$ kubectl get nodes
	$ kubectl cluster-info
\end{verbatim}

\section{Openstack Resource Specification}
To get a full understanding of the Openstack resources, 
which are provisioned by Terraform,
you have to look at the terraform plan files \verb|*.tf| in the
\verb|terraform/| directory.

\subsection{Resource Overview}
Firstly, a private network is created with a subnet,
which is where the compute instances are located. It is only a private network,
in the sense that machines do not have public ip addresses. For this purpose,
Floating IP addresses are created for each compute instance. Furthermore,
a security group is created with multiple ingress rules. 

\begin{itemize}
	\item Ingress Port 22 - SSH
	\item Ingress Port 6443 - Kubernetes API
\end{itemize}

\noindent
The SSH port is needed, in order for Ansible to connect.
The Kubernetes API port is needed for the master (control plane) node,
if the operator wants to manage the cluster from their own workstation. \\

\noindent
The security group can and should be optimized, by specifying the remaining
rules, which are needed by Kubernetes for in-cluster traffic.
For the sake of 
simplicity, the same security group is used for the Kubernetes Master and Node.
This should be sorted out before using this project in production. \\

\noindent
A router is used to connect the private network with the provider network,
which is where the Floating IP addresses are created. 
This network is accessible from the local area network (LAN),
where the Openstack server is located. 
Another much needed resource is the public ssh key of the operator,
which will be imported into Openstack by Terraform. 
Finally, two groups of Compute Instances are created. 
The \verb|kube_master| and the \verb|kube_node| resources.
By default, one \verb|kube_master| instance and two \verb|kube_node| instances.
The instances are installed from a \verb|Ubuntu-20.04| image and the 
Openstack Nova instance type is \verb|m1.big|.

